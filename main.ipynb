import numpy as np
import pandas as pd
import statsmodels.api as sm import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report
from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier from lightgbm import LGBMClassifier
from sklearn.model_selection import KFold import warnings warnings.simplefilter(action = "ignore")
df = pd.read_csv("/diabetes.csv") df.head()
df.shape df.info()
df.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T
df["Outcome"].value_counts()*100/len(df) df.Outcome.value_counts() df["Age"].hist(edgecolor = "black");
print("Max Age: " + str(df["Age"].max()) + " Min Age: " + str(df["Age"].min()))

# Histogram and density graphs of all variables are accessed. fig, ax = plt.subplots(4,2, figsize=(16,16))
sns.distplot(df.Age, bins = 20, ax=ax[0,0]) sns.distplot(df.Pregnancies, bins = 20, ax=ax[0,1]) sns.distplot(df.Glucose, bins = 20, ax=ax[1,0])
 
sns.distplot(df.BloodPressure, bins = 20, ax=ax[1,1]) sns.distplot(df.SkinThickness, bins = 20, ax=ax[2,0]) sns.distplot(df.Insulin, bins = 20, ax=ax[2,1]) sns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) sns.distplot(df.BMI, bins = 20, ax=ax[3,1])
df.groupby("Outcome").agg({"Pregnancies":"mean"})

df.groupby("Outcome").agg({"Age":"mean"})

df.groupby("Outcome").agg({"Age":"max"})
df.groupby("Outcome").agg({"Insulin": "mean"})

df.groupby("Outcome").agg({"Insulin": "max"})
df.groupby("Outcome").agg({"Glucose": "mean"})

df.groupby("Outcome").agg({"Glucose": "max"})

df.groupby("Outcome").agg({"BMI": "mean"})

# The distribution of the outcome variable in the data was examined and visualized. f,ax=plt.subplots(1,2,figsize=(18,8)) df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow
=True) ax[0].set_title('target') ax[0].set_ylabel('')
sns.countplot(x='Outcome',data=df,ax=ax[1]) ax[1].set_title('Outcome')
plt.show() df.corr()
# Correlation matrix graph
f, ax = plt.subplots(figsize= [20,15])
sns.heatmap(df.corr(), annot=True, fmt=".2f", ax=ax, cmap = "magma" ) ax.set_title("Correlation Matrix", fontsize=20)
plt.show()

"""# 2) Data Preprocessing
## 2.1) Missing Observation Analysis
We saw on df.head() that some features contain 0, it doesn't make sense here and this indicates missing value Below we replace 0 value by NaN:
"""
 
df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)
df.head() df.isnull().sum()
import missingno as msno msno.bar(df);
# The missing values filled with the median values def median_target(var):
temp = df[df[var].notnull()]
temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index() return temp
columns = df.columns
columns = columns.drop("Outcome") for i in columns:
median_target(i)
df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]
df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1] df.head()
# Missing values are filled. df.isnull().sum()
"""## 2.2) Outlier Observation Analysis"""

# In the data set, there were asked whether there were any outlier observations compared to the 25% and 75% quarters.
# It was found to be an outlier observation. for feature in df:

Q1 = df[feature].quantile(0.25) Q3 = df[feature].quantile(0.75)
IQR = Q3-Q1
lower = Q1- 1.5*IQR upper = Q3 + 1.5*IQR
if df[(df[feature] > upper)].any(axis=None): print(feature,"yes")
else:
print(feature, "no")
 
# visualizing the Insulin variable with boxplot method sns.boxplot(x = df["Insulin"]);
#We conduct a stand alone observation review for the Insulin variable #We suppress contradictory values
Q1 = df.Insulin.quantile(0.25) Q3 = df.Insulin.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR upper = Q3 + 1.5*IQR
df.loc[df["Insulin"] > upper,"Insulin"] = upper upper
import seaborn as sns sns.boxplot(x = df["Insulin"]);
"""## 2.3) Local Outlier Factor (LOF)"""
# We determine outliers between all variables with the LOF method from sklearn.neighbors import LocalOutlierFactor
lof =LocalOutlierFactor(n_neighbors= 10) lof.fit_predict(df)
df_scores = lof.negative_outlier_factor_ #df_scores
np.sort(df_scores)[0:30]
#choose the threshold value according to lof scores threshold = np.sort(df_scores)[7]
threshold
#We delete those that are higher than the threshold outlier = df_scores > threshold
df = df[outlier]

# The size of the data set was examined. df.shape

"""# 3) Feature Engineering

Creating new variables is important for models. But you need to create a logical new variable. For this data set, some new variables were created according to BMI, Insulin and glucose variables.
"""
# According to BMI, some ranges were determined and categorical variables were assigned.
 
NewBMI = pd.Series(["Underweight", "Normal", "Overweight", "Obesity 1", "Obesity 2", "Obesity 3"], dtype = "category")
df["NewBMI"] = NewBMI
df.loc[df["BMI"] < 18.5, "NewBMI"] = NewBMI[0]
df.loc[(df["BMI"] > 18.5) & (df["BMI"] <= 24.9), "NewBMI"] = NewBMI[1]
df.loc[(df["BMI"] > 24.9) & (df["BMI"] <= 29.9), "NewBMI"] = NewBMI[2]
df.loc[(df["BMI"] > 29.9) & (df["BMI"] <= 34.9), "NewBMI"] = NewBMI[3]
df.loc[(df["BMI"] > 34.9) & (df["BMI"] <= 39.9), "NewBMI"] = NewBMI[4] df.loc[df["BMI"] > 39.9 ,"NewBMI"] = NewBMI[5]
df.head()
# A categorical variable creation process is performed according to the insulin value. def set_insulin(row):
if row["Insulin"] >= 16 and row["Insulin"] <= 166: return "Normal"
else:
return "Abnormal"
# The operation performed was added to the dataframe.
df = df.assign(NewInsulinScore=df.apply(set_insulin, axis=1)) df.head()
# Some intervals were determined according to the glucose variable and these were assigned categorical variables.
NewGlucose = pd.Series(["Low", "Normal", "Overweight", "Secret", "High"], dtype = "category")
df["NewGlucose"] = NewGlucose
df.loc[df["Glucose"] <= 70, "NewGlucose"] = NewGlucose[0]
df.loc[(df["Glucose"] > 70) & (df["Glucose"] <= 99), "NewGlucose"] = NewGlucose[1] df.loc[(df["Glucose"] > 99) & (df["Glucose"] <= 126), "NewGlucose"] = NewGlucose[2] df.loc[df["Glucose"] > 126 ,"NewGlucose"] = NewGlucose[3]
df.head()

"""# 4) One Hot Encoding

Categorical variables in the data set should be converted into numerical values. For this reason, these transformation processes are performed with Label Encoding and One Hot Encoding method.
"""
df = pd.get_dummies(df, columns =["NewBMI","NewInsulinScore", "NewGlucose"], drop_first = True)
df.head()
 
categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',
'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]
categorical_df.head() y = df["Outcome"]
X = df.drop(["Outcome",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3',
'NewBMI_Overweight','NewBMI_Underweight', 'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal',
'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1) cols = X.columns
index = X.index X.head()
from sklearn.preprocessing import RobustScaler transformer = RobustScaler().fit(X)
X = transformer.transform(X)
X = pd.DataFrame(X, columns = cols, index = index) X.head()
X = pd.concat([X,categorical_df], axis = 1) X.head(10)
y.head()
"""# 5) Base Models"""
# Validation scores of all base models models = []
models.append(('LR', LogisticRegression(random_state = 12345))) models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier(random_state = 12345))) models.append(('RF', RandomForestClassifier(random_state = 12345))) models.append(('SVM', SVC(gamma='auto', random_state = 12345))) models.append(('XGB', GradientBoostingClassifier(random_state = 12345))) models.append(("LightGBM", LGBMClassifier(random_state = 12345)))
# evaluate each model in turn results = []
names = []
 
for name, model in models:
kfold = KFold(n_splits = 10, random_state = 12345, shuffle=True) cv_results = cross_val_score(model, X, y, cv = 10, scoring= "accuracy") results.append(cv_results)
names.append(name)
msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std()) print(msg)
# boxplot algorithm comparison fig = plt.figure(figsize=(10,5))
fig.suptitle('Algorithm Comparison') ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show()
"""# 6) Model Tuning
### 1) Random Forests Tuning """

rf_params = {"n_estimators" :[100,200,500,1000], "max_features": [3,5,7],
"min_samples_split": [2,5,10,30], "max_depth": [3,5,8,None]}
rf_model = RandomForestClassifier(random_state = 12345) gs_cv = GridSearchCV(rf_model,
rf_params, cv = 10,
n_jobs = -1, verbose = 2).fit(X, y)

gs_cv.best_params_

"""### 1.1) Final Model Installation"""
rf_tuned = RandomForestClassifier(**gs_cv.best_params_) rf_tuned = rf_tuned.fit(X,y)
cross_val_score(rf_tuned, X, y, cv = 10).mean() feature_imp = pd.Series(rf_tuned.feature_importances_,
 
index=X.columns).sort_values(ascending=False)

sns.barplot(x=feature_imp, y=feature_imp.index) plt.xlabel('Significance Score Of Variables') plt.ylabel('Variables')
plt.title("Variable Severity Levels") plt.show()

"""### 2) LightGBM Tuning"""
lgbm = LGBMClassifier(random_state = 12345) lgbm_params = {"learning_rate": [0.01, 0.03, 0.05, 0.1, 0.5],
"n_estimators": [500, 1000, 1500], "max_depth":[3,5,8]}
gs_cv = GridSearchCV(lgbm,
lgbm_params, cv = 10,
n_jobs = -1, verbose = 2).fit(X, y)

gs_cv.best_params_
"""### 2.1) Final Model Installation"""
lgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X,y) cross_val_score(lgbm_tuned, X, y, cv = 10).mean() feature_imp = pd.Series(lgbm_tuned.feature_importances_,
index=X.columns).sort_values(ascending=False)

sns.barplot(x=feature_imp, y=feature_imp.index) plt.xlabel('Significance Score Of Variables') plt.ylabel('Variables')
plt.title("Variable Severity Levels") plt.show()

"""### 3) XGBoost Tuning"""
xgb = GradientBoostingClassifier(random_state = 12345) xgb_params = {
"learning_rate": [0.01, 0.1, 0.2, 1],
"min_samples_split": np.linspace(0.1, 0.5, 10), "max_depth":[3,5,8],
 
"subsample":[0.5, 0.9, 1.0],
"n_estimators": [100,1000]}
xgb_cv_model = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X, y) xgb_cv_model.best_params_
"""### 3.1) Final Model Installation"""

xgb_tuned = GradientBoostingClassifier(**xgb_cv_model.best_params_).fit(X,y) cross_val_score(xgb_tuned, X, y, cv = 10).mean()
feature_imp = pd.Series(xgb_tuned.feature_importances_, index=X.columns).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index) plt.xlabel('Significance Score Of Variables') plt.ylabel('Variables')
plt.title("Variable Severity Levels") plt.show()

"""# 7) Comparison of Final Models""" models = []
models.append(('RF', RandomForestClassifier(random_state = 12345, max_depth = 8, max_features = 7, min_samples_split = 2, n_estimators = 500)))
models.append(('XGB', GradientBoostingClassifier(random_state = 12345, learning_rate = 0.1, max_depth = 5, min_samples_split = 0.1, n_estimators = 100, subsample = 1.0))) models.append(("LightGBM", LGBMClassifier(random_state = 12345, learning_rate = 0.01, max_depth = 3, n_estimators = 1000)))
# evaluate each model in turn results = []
names = []

for name, model in models:
kfold = KFold(n_splits = 10, random_state = 12345,shuffle=True) cv_results = cross_val_score(model, X, y, cv = 10, scoring= "accuracy") results.append(cv_results)
names.append(name)
msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std()) print(msg)
# boxplot algorithm comparison
 
fig = plt.figure(figsize=(10,5)) fig.suptitle('Algorithm Comparison') ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show()
